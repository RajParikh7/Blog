{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "import time,math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\" , 2: \"PAD\" , 3:\"UNK\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4490\n",
      "eng 2926\n",
      "['vous commettez une grosse erreur .', 'you re making a big mistake .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, filter_pair=True):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    if filter_pair:\n",
    "        pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, max_len ):\n",
    "    arr = [PAD_token]*max_len\n",
    "    \n",
    "    for idx,word in enumerate( sentence.split(' ') ):\n",
    "        arr[idx] = lang.word2index[word]\n",
    "        \n",
    "    arr[idx+1] = EOS_token\n",
    "    return arr\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence, max_len):\n",
    "    indexes = indexesFromSentence(lang, sentence,max_len)\n",
    "    result = torch.LongTensor(indexes)\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair, input_lang, output_lang, in_max, out_max ):\n",
    "    input_variable = variableFromSentence( input_lang, pair[0], in_max)\n",
    "    target_variable = variableFromSentence( output_lang, pair[1], out_max)\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lang2lang( Dataset ):\n",
    "    \n",
    "    def __init__( self, pairs, input_lang, output_lang, in_max, out_max ):\n",
    "        super(lang2lang,self).__init__()\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.in_max = in_max\n",
    "        self.out_max = out_max\n",
    "    \n",
    "    def __len__( self ):    \n",
    "        return len( self.pairs )\n",
    "    \n",
    "    def __getitem__( self, idx ):\n",
    "        \n",
    "        input_variable=variableFromSentence(self.input_lang, self.pairs[ idx][0], self.in_max)\n",
    "        target_variable=variableFromSentence(self.output_lang, self.pairs[ idx][1], self.out_max)\n",
    "        \n",
    "        return { \"input\":input_variable , \"target\":target_variable }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = lang2lang( pairs, input_lang, output_lang, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader( dataset,batch_size=16,shuffle=False,num_workers=4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model and sublayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax3D( nn.Module ):\n",
    "    \n",
    "    def __init__( self ):\n",
    "        super( Softmax3D, self ).__init__()\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        s0,s1,s2 = x.size()\n",
    "        return F.softmax( x.view(s0*s1,s2) ).view( s0,s1,s2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaledAttention( nn.Module ):\n",
    "    '''Scaled dot product attention mechanism basis of Attention is all you need\n",
    "       three things required query key value,\n",
    "       key and value are always same,\n",
    "       when self attention query is also the same\n",
    "       when not query is from the decoder'''\n",
    "    \n",
    "    def __init__( self, d , dropout=0.1 , n_head=1):\n",
    "        \n",
    "        super( ScaledAttention, self).__init__()\n",
    "        self.scale = np.power(d, 0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = Softmax3D()\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward( self , query , key , value , attn_mask=None ):\n",
    "        attn_val = torch.bmm( query , key.transpose(1,2) ) / self.scale\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            attn_val.data.masked_fill_( attn_mask.repeat(self.n_head,1,1) , -float('inf') )\n",
    "        \n",
    "        attn_val = self.softmax(attn_val)\n",
    "        attn_val = self.dropout(attn_val)\n",
    "        output   = torch.bmm( attn_val, value )\n",
    "        \n",
    "        return output , attn_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention( nn.Module ):\n",
    "    \n",
    "    def __init__( self , d_model, n_head , dropout=0.1):\n",
    "        super( MultiHeadAttention, self ).__init__()\n",
    "        \n",
    "        assert d_model%n_head == 0 , \"n_head({}) must divide d_model({})\".format(n_head,d_model) \n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_intermediate = self.d_model // self.n_head\n",
    "        \n",
    "        self.start_proj_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.start_proj_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.start_proj_v = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        self.attention = ScaledAttention(self.d_intermediate, dropout, n_head)\n",
    "        \n",
    "        self.proj = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward( self , query , key , value , attn_mask=None ):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        query = self.start_proj_q( query )\n",
    "        key   = self.start_proj_k( key )\n",
    "        value = self.start_proj_v( value )\n",
    "        \n",
    "        query = torch.cat( query.split( self.d_intermediate,-1 ) , 0)\n",
    "        key   = torch.cat(   key.split( self.d_intermediate,-1 ) , 0)\n",
    "        value = torch.cat( value.split( self.d_intermediate,-1 ) , 0)\n",
    "        \n",
    "        output, attn = self.attention(query, key, value, attn_mask )\n",
    "        output = torch.cat( output.split( batch_size,0 ) , -1)\n",
    "        \n",
    "        output = self.proj( output )\n",
    "        output = self.dropout( output )\n",
    "        \n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward( nn.Module ):\n",
    "\n",
    "    def __init__(self, d_model, d_inner=None, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        if d_inner==None:\n",
    "            d_inner = 4*d_model\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, d_inner) \n",
    "        self.proj_out = nn.Linear(d_inner, d_model) \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = F.relu( self.proj(x) )\n",
    "        output = self.proj_out(output)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, eps=1e-3):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model), requires_grad=True)\n",
    "        self.offset = nn.Parameter(torch.zeros(d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.size(1) == 1:\n",
    "            return z\n",
    "\n",
    "        mu = torch.mean(z, keepdim=True, dim=-1)\n",
    "        sigma = torch.std(z, keepdim=True, dim=-1)\n",
    "        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n",
    "        ln_out = ln_out * self.scale.expand_as(ln_out) + self.offset.expand_as(ln_out)\n",
    "\n",
    "        return ln_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \n",
    "    def __init__( self, d_model, n_head ,dropout=0.1 ):\n",
    "        \n",
    "        super( EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention( d_model, n_head, dropout )\n",
    "        self.feedforward = PositionwiseFeedForward( d_model, dropout=dropout )\n",
    "        \n",
    "        self.layer_normalization_att = LayerNormalization(d_model)\n",
    "        self.layer_normalization_feed = LayerNormalization(d_model)\n",
    "        \n",
    "    def forward( self, enc_input, self_attn_mask=None ):\n",
    "        \n",
    "        enc_out, _ = self.attention(enc_input,enc_input,enc_input,self_attn_mask )\n",
    "        enc_out = self.layer_normalization_att( enc_out + enc_input )\n",
    "        \n",
    "        enc_out = self.layer_normalization_feed( self.feedforward(enc_out) + enc_out )\n",
    "        return enc_out , _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model, n_head, dropout=dropout)\n",
    "        self.enc_attention = MultiHeadAttention(d_model, n_head, dropout=dropout)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, dropout=dropout)\n",
    "        \n",
    "        self.layer_normalization_self = LayerNormalization(d_model)\n",
    "        self.layer_normalization_enc  = LayerNormalization(d_model)\n",
    "        self.layer_normalization_feed = LayerNormalization(d_model)\n",
    "        \n",
    "    def forward(self, dec_input, enc_output, self_attn_mask=None, enc_attn_mask=None):\n",
    "        \n",
    "        dec_output, _ = self.self_attention(dec_input, dec_input, dec_input, attn_mask=self_attn_mask)\n",
    "        dec_input = self.layer_normalization_self( dec_input + dec_output )\n",
    "        \n",
    "        dec_output, __ = self.enc_attention(dec_input, enc_output, enc_output, attn_mask=enc_attn_mask)\n",
    "        dec_output = self.layer_normalization_enc( dec_input + dec_output )\n",
    "        \n",
    "        dec_output = self.layer_normalization_feed( self.feedforward(dec_output) + dec_output )\n",
    "\n",
    "        return dec_output, _ , __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    position_enc = np.array( [ [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n",
    "                                if pos != 0 else np.zeros(d_pos_vec) \n",
    "                                for pos in range(n_position) ] )\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "\n",
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    ''' Indicate the padding-related part to mask '''\n",
    "    \n",
    "    assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
    "    \n",
    "    mb_size, len_q = seq_q.size()\n",
    "    mb_size, len_k = seq_k.size()\n",
    "    \n",
    "    pad_attn_mask = seq_k.data.eq( PAD_token ).unsqueeze(1)   # bx1xsk\n",
    "    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k) # bxsqxsk\n",
    "    \n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    ''' Get an attention mask to avoid using the subsequent info.'''\n",
    "    \n",
    "    assert seq.dim() == 2\n",
    "    \n",
    "    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "    \n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__( self, src_vocab , max_seq_len, n_layers=6, n_head=8, d_word_vec=512, d_model=512, dropout=0.1):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.position_enc = nn.Embedding( max_seq_len+1, d_word_vec )\n",
    "        self.position_enc.weight.data = position_encoding_init(max_seq_len+1, d_word_vec)\n",
    "\n",
    "        self.src_word_emb = nn.Embedding( src_vocab, d_word_vec, padding_idx=PAD_token)\n",
    "\n",
    "        self.layer = nn.ModuleList( [EncoderLayer(d_model, n_head, dropout=dropout) for _ in range(n_layers)] )\n",
    "\n",
    "    def forward(self, src_seq, src_pos):\n",
    "       \n",
    "        enc_input = self.src_word_emb(src_seq)\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "        \n",
    "        enc_outputs, enc_self_attns = [], []\n",
    "        enc_output = enc_input\n",
    "        \n",
    "        self_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
    "        \n",
    "        for enc_layer in self.layer:\n",
    "            \n",
    "            enc_output, enc_self_attn = enc_layer( enc_output, self_attn_mask=self_attn_mask)\n",
    "    \n",
    "            enc_outputs.append( enc_output )\n",
    "            enc_self_attns.append( enc_self_attn )\n",
    "\n",
    "        return enc_outputs, enc_self_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__( self, tgt_vocab, max_seq_len, n_layers=6, n_head=8, d_word_vec=512, d_model=512, dropout=0.1):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.position_enc = nn.Embedding( max_seq_len+1, d_word_vec )\n",
    "        self.position_enc.weight.data = position_encoding_init( max_seq_len+1, d_word_vec)\n",
    "\n",
    "        self.tgt_word_emb = nn.Embedding( tgt_vocab, d_word_vec, padding_idx=PAD_token )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer = nn.ModuleList( [ DecoderLayer(d_model, n_head, dropout=dropout) for _ in range(n_layers) ] )\n",
    "\n",
    "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_outputs):\n",
    "        \n",
    "        dec_input = self.tgt_word_emb(tgt_seq)\n",
    "        dec_input += self.position_enc(tgt_pos)\n",
    "\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = [], [], []\n",
    "\n",
    "        dec_self_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)\n",
    "        dec_self_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)\n",
    "        \n",
    "        dec_self_attn_mask     = torch.gt(dec_self_attn_pad_mask + dec_self_attn_sub_mask, 0)\n",
    "        dec_enc_attn_pad_mask  = get_attn_padding_mask(tgt_seq, src_seq)\n",
    "\n",
    "        dec_output = dec_input\n",
    "        \n",
    "        for dec_layer, enc_output in zip(self.layer, enc_outputs):\n",
    "            \n",
    "            dec_output, dec_self_attn, dec_enc_attn = dec_layer( dec_output, enc_output, \n",
    "                                                                 self_attn_mask = dec_self_attn_mask,\n",
    "                                                                 enc_attn_mask  = dec_enc_attn_pad_mask )\n",
    "\n",
    "            dec_outputs.append( dec_output )\n",
    "            dec_self_attns.append( dec_self_attn )\n",
    "            dec_enc_attns.append( dec_enc_attn )\n",
    "\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ''' Attention is all you need '''\n",
    "\n",
    "    def __init__( self, src_vocab, tgt_vocab, max_seq_len, n_layers=6, n_head=8, d_word_vec=512, d_model=512,\n",
    "                  dropout=0.1, embs_share_weight=False):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        assert d_model == d_word_vec,'To facilitate the residual connections.'\n",
    "        \n",
    "        self.encoder = Encoder( src_vocab, max_seq_len, n_layers=n_layers, n_head=n_head,\n",
    "                                d_word_vec=d_word_vec, d_model=d_model, dropout=dropout )\n",
    "        \n",
    "        self.decoder = Decoder( tgt_vocab, max_seq_len, n_layers=n_layers, n_head=n_head,\n",
    "                                d_word_vec=d_word_vec, d_model=d_model, dropout=dropout)\n",
    "        \n",
    "        self.tgt_word_proj = nn.Linear(d_model, tgt_vocab, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if embs_share_weight:\n",
    "            assert src_vocab == tgt_vocab,\"To share word embedding,The vocabulary size of src/tgt must be same\"\n",
    "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        ''' Avoid updating the position encoding '''\n",
    "        \n",
    "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n",
    "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n",
    "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n",
    "        \n",
    "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq, src_pos = src\n",
    "        tgt_seq, tgt_pos = tgt\n",
    "\n",
    "        enc_outputs, enc_slf_attns = self.encoder(src_seq, src_pos)\n",
    "        dec_outputs, dec_slf_attns, dec_enc_attns = self.decoder( tgt_seq, tgt_pos, src_seq, enc_outputs)\n",
    "        dec_output = dec_outputs[-1]\n",
    "        \n",
    "        seq_logit = self.tgt_word_proj(dec_output)\n",
    "        \n",
    "        sz = seq_logit.size()\n",
    "        log_loss = F.log_softmax( seq_logit.view(-1, seq_logit.size(2)) )\n",
    "        return log_loss.view( sz )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.75\n",
    "\n",
    "def train(input_variable, target_variable, model, optimizer, criterion, max_seq_len ):\n",
    "    \n",
    "    batch_size = input_variable.size(0)\n",
    "    \n",
    "    input_pos = Variable( torch.LongTensor( range(1,max_seq_len+1) ).view(1,-1).repeat(batch_size,1) )\n",
    "    target_pos = input_pos.clone()\n",
    "\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        teacher_forcing_target = Variable( torch.LongTensor([[SOS_token]]*batch_size ) )\n",
    "        teacher_forcing_target = torch.cat( ( teacher_forcing_target,target_variable[:,:-1]), dim=1)\n",
    "\n",
    "        output = model( (input_variable,input_pos) , (teacher_forcing_target,target_pos) )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        output , output_generated = eval( input_variable, max_seq_len , model )\n",
    "                \n",
    "    loss = criterion( output.view(-1,output.size(2) ), target_variable.view(-1) )\n",
    "\n",
    "    _ , max_index = output.data.topk(1)\n",
    "    correct = ( max_index.view(-1) == target_variable.data.view(-1) ).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / 10 , correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(input_variable, max_seq_len, model ):\n",
    "    \n",
    "    batch_size = input_variable.size(0)\n",
    "    \n",
    "    input_pos  = Variable( torch.LongTensor( range(1,max_seq_len+1) ).view(1,-1).repeat(batch_size,1) )\n",
    "    target_pos = input_pos.clone()\n",
    "    \n",
    "    target_var = Variable( torch.LongTensor( [[PAD_token]]*batch_size ).repeat(1,max_seq_len) )\n",
    "    target_var[:,0] = SOS_token\n",
    "\n",
    "    for index in range(1,max_seq_len):\n",
    "\n",
    "        output = model( (input_variable,input_pos ), (target_var,target_pos ) )\n",
    "        correct_indices = output.topk(1)[1].squeeze(2)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            PADDING_REQ = (target_var[i,index-1].data[0]==EOS_token or target_var[i,index-1].data[0]==PAD_token)\n",
    "            target_var[i,index] = PAD_token if PADDING_REQ else correct_indices[i,index-1]  \n",
    "            \n",
    "    # we return the output to be used by the train instance\n",
    "    # and the target_var \n",
    "    target_var = output.topk(1)[1].squeeze(2)\n",
    "        \n",
    "    return output, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(model, max_seq_len, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "\n",
    "    optimizer = optim.Adam( model.get_trainable_parameters() ,lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
    "    \n",
    "    plot_losses = []\n",
    "    \n",
    "    for epoch in range(100):    \n",
    "        \n",
    "        print_loss_total = 0  \n",
    "        plot_loss_total = 0  \n",
    "\n",
    "        for iter,X in enumerate(dataloader):\n",
    "\n",
    "            input_variable  = Variable( X['input'] )\n",
    "            target_variable = Variable( X['target'] )\n",
    "\n",
    "            loss , correct = train(input_variable, target_variable, model, optimizer, criterion,\n",
    "                                   max_seq_len , epoch==99)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total  += loss\n",
    "            \n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                \n",
    "                if iter==0:\n",
    "                    print_loss_avg *= print_every\n",
    "                print_loss_total = 0\n",
    "                print('%d: %.4f' % ( epoch,  print_loss_avg))\n",
    "                total = target_variable.data.ne(PAD_token).sum()\n",
    "                print(\"current batch correct are: {}/{} = {:.6f}\".format(correct,total,correct/total) )\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / print_every\n",
    "                \n",
    "                if iter==0:\n",
    "                    plot_loss_avg *= print_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "#             here training only a single batch of data \n",
    "#             because dont want to blow up my computer\n",
    "            \n",
    "#             if iter == 0:\n",
    "#                 break\n",
    "                \n",
    "    plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.8168\n",
      "current batch correct are: 0/79 = 0.000000\n",
      "1: 0.7522\n",
      "current batch correct are: 0/79 = 0.000000\n",
      "2: 0.7194\n",
      "current batch correct are: 2/79 = 0.025316\n",
      "3: 0.6991\n",
      "current batch correct are: 16/79 = 0.202532\n",
      "4: 0.6799\n",
      "current batch correct are: 3/79 = 0.037975\n",
      "5: 0.6586\n",
      "current batch correct are: 16/79 = 0.202532\n",
      "6: 0.6354\n",
      "current batch correct are: 17/79 = 0.215190\n",
      "7: 0.6105\n",
      "current batch correct are: 18/79 = 0.227848\n",
      "8: 0.5844\n",
      "current batch correct are: 18/79 = 0.227848\n",
      "9: 0.5585\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "10: 0.5347\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "11: 0.5103\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "12: 0.4849\n",
      "current batch correct are: 33/79 = 0.417722\n",
      "13: 0.4593\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "14: 0.4329\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "15: 0.4058\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "16: 0.3790\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "17: 0.3532\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "18: 0.3283\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "19: 0.3039\n",
      "current batch correct are: 31/79 = 0.392405\n",
      "20: 0.2805\n",
      "current batch correct are: 31/79 = 0.392405\n",
      "21: 0.2586\n",
      "current batch correct are: 31/79 = 0.392405\n",
      "22: 0.2383\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "23: 0.2196\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "24: 0.2026\n",
      "current batch correct are: 32/79 = 0.405063\n",
      "25: 0.1871\n",
      "current batch correct are: 40/79 = 0.506329\n",
      "26: 0.1731\n",
      "current batch correct are: 53/79 = 0.670886\n",
      "27: 0.1604\n",
      "current batch correct are: 57/79 = 0.721519\n",
      "28: 0.1486\n",
      "current batch correct are: 58/79 = 0.734177\n",
      "29: 0.1375\n",
      "current batch correct are: 61/79 = 0.772152\n",
      "30: 0.1274\n",
      "current batch correct are: 62/79 = 0.784810\n",
      "31: 0.1181\n",
      "current batch correct are: 63/79 = 0.797468\n",
      "32: 0.1097\n",
      "current batch correct are: 64/79 = 0.810127\n",
      "33: 0.1019\n",
      "current batch correct are: 67/79 = 0.848101\n",
      "34: 0.0944\n",
      "current batch correct are: 68/79 = 0.860759\n",
      "35: 0.0878\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "36: 0.0829\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "37: 0.0790\n",
      "current batch correct are: 67/79 = 0.848101\n",
      "38: 0.0733\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "39: 0.0697\n",
      "current batch correct are: 68/79 = 0.860759\n",
      "40: 0.0633\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "41: 0.0614\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "42: 0.0562\n",
      "current batch correct are: 69/79 = 0.873418\n",
      "43: 0.0531\n",
      "current batch correct are: 71/79 = 0.898734\n",
      "44: 0.0506\n",
      "current batch correct are: 71/79 = 0.898734\n",
      "45: 0.0469\n",
      "current batch correct are: 72/79 = 0.911392\n",
      "46: 0.0442\n",
      "current batch correct are: 73/79 = 0.924051\n",
      "47: 0.0419\n",
      "current batch correct are: 73/79 = 0.924051\n",
      "48: 0.0384\n",
      "current batch correct are: 74/79 = 0.936709\n",
      "49: 0.0366\n",
      "current batch correct are: 75/79 = 0.949367\n",
      "50: 0.0343\n",
      "current batch correct are: 75/79 = 0.949367\n",
      "51: 0.0317\n",
      "current batch correct are: 75/79 = 0.949367\n",
      "52: 0.0302\n",
      "current batch correct are: 76/79 = 0.962025\n",
      "53: 0.0281\n",
      "current batch correct are: 75/79 = 0.949367\n",
      "54: 0.0267\n",
      "current batch correct are: 77/79 = 0.974684\n",
      "55: 0.0252\n",
      "current batch correct are: 76/79 = 0.962025\n",
      "56: 0.0237\n",
      "current batch correct are: 77/79 = 0.974684\n",
      "57: 0.0221\n",
      "current batch correct are: 77/79 = 0.974684\n",
      "58: 0.0203\n",
      "current batch correct are: 77/79 = 0.974684\n",
      "59: 0.0194\n",
      "current batch correct are: 77/79 = 0.974684\n",
      "60: 0.0182\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "61: 0.0169\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "62: 0.0160\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "63: 0.0150\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "64: 0.0141\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "65: 0.0133\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "66: 0.0125\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "67: 0.0117\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "68: 0.0112\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "69: 0.0105\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "70: 0.0099\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "71: 0.0095\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "72: 0.0090\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "73: 0.0085\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "74: 0.0081\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "75: 0.0078\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "76: 0.0074\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "77: 0.0071\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "78: 0.0069\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "79: 0.0066\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "80: 0.0063\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "81: 0.0061\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "82: 0.0059\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "83: 0.0057\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "84: 0.0055\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "85: 0.0054\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "86: 0.0052\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "87: 0.0051\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "88: 0.0050\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "89: 0.0048\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "90: 0.0047\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "91: 0.0046\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "92: 0.0045\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "93: 0.0044\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "94: 0.0044\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "95: 0.0043\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "96: 0.0042\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "97: 0.0041\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "98: 0.0041\n",
      "current batch correct are: 78/79 = 0.987342\n",
      "Variable containing:\n",
      "    3     4     5     1     5     5     5     5     5     5\n",
      "    3     4     6     5     1     5     5     6     5     5\n",
      "    3     4     6     5     1     5     5     6     5     5\n",
      "    3     4     7     5     1     5     1     1     1     5\n",
      "    3     4     7     5     1     5     1     1     1     5\n",
      "    3     4     8     5     1     5     5     5     5     5\n",
      "    3     4     9    10     1     5     5     9     5     5\n",
      "    3     4     9    10     1     5     5     5     5     5\n",
      "    3     4    11     5     1     5     1     7     1     5\n",
      "    3     4    12     5     1     5     5    18     5     5\n",
      "    3     4    13     5     1     5     7    18     5     5\n",
      "    3     4    14     5     1     5     5     9     5     5\n",
      "    3     4    14     5     1     5     5     5     5     5\n",
      "   15    16    14     5     1    15     8     8     8     5\n",
      "    3     4     7     5     1     5     1     7     1     5\n",
      "    3     4    18     5     1     5     5     7     5     5\n",
      "[torch.LongTensor of size 16x10]\n",
      "\n",
      "Variable containing:\n",
      "    3     4     5     1     2     2     2     2     2     2\n",
      "    3     4     6     5     1     2     2     2     2     2\n",
      "    3     4     6     5     1     2     2     2     2     2\n",
      "    3     4     7     5     1     2     2     2     2     2\n",
      "    3     4     7     5     1     2     2     2     2     2\n",
      "    3     4     8     5     1     2     2     2     2     2\n",
      "    3     4     9    10     1     2     2     2     2     2\n",
      "    3     4     9    10     1     2     2     2     2     2\n",
      "    3     4    11     5     1     2     2     2     2     2\n",
      "    3     4    12     5     1     2     2     2     2     2\n",
      "    3     4    13     5     1     2     2     2     2     2\n",
      "    3     4    14     5     1     2     2     2     2     2\n",
      "    3     4    14     5     1     2     2     2     2     2\n",
      "   15    16    14     5     1     2     2     2     2     2\n",
      "    3    17     7     5     1     2     2     2     2     2\n",
      "    3     4    18     5     1     2     2     2     2     2\n",
      "[torch.LongTensor of size 16x10]\n",
      "\n",
      "99: 0.0040\n",
      "current batch correct are: 78/79 = 0.987342\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKdxFRDKQwl1EJAMp3EVEMpDCXUQkA/1/vR7cotstXqEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea70a524a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Transformer( input_lang.n_words, output_lang.n_words, max_seq_len=10, n_layers=2, n_head=2,\n",
    "                     d_word_vec = 12, d_model = 12, dropout = 0 )\n",
    "\n",
    "trainIters(model, 10, print_every=500 ,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer (\n",
       "  (encoder): Encoder (\n",
       "    (position_enc): Embedding(11, 12)\n",
       "    (src_word_emb): Embedding(4490, 12, padding_idx=2)\n",
       "    (layer): ModuleList (\n",
       "      (0): EncoderLayer (\n",
       "        (attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedForward (\n",
       "          (proj): Linear (12 -> 48)\n",
       "          (proj_out): Linear (48 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (layer_normalization_att): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_feed): LayerNormalization (\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer (\n",
       "        (attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedForward (\n",
       "          (proj): Linear (12 -> 48)\n",
       "          (proj_out): Linear (48 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (layer_normalization_att): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_feed): LayerNormalization (\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder (\n",
       "    (position_enc): Embedding(11, 12)\n",
       "    (tgt_word_emb): Embedding(2926, 12, padding_idx=2)\n",
       "    (dropout): Dropout (p = 0)\n",
       "    (layer): ModuleList (\n",
       "      (0): DecoderLayer (\n",
       "        (self_attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (enc_attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedForward (\n",
       "          (proj): Linear (12 -> 48)\n",
       "          (proj_out): Linear (48 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (layer_normalization_self): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_enc): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_feed): LayerNormalization (\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer (\n",
       "        (self_attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (enc_attention): MultiHeadAttention (\n",
       "          (start_proj_q): Linear (12 -> 12)\n",
       "          (start_proj_k): Linear (12 -> 12)\n",
       "          (start_proj_v): Linear (12 -> 12)\n",
       "          (attention): ScaledAttention (\n",
       "            (dropout): Dropout (p = 0)\n",
       "            (softmax): Softmax3D (\n",
       "            )\n",
       "          )\n",
       "          (proj): Linear (12 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedForward (\n",
       "          (proj): Linear (12 -> 48)\n",
       "          (proj_out): Linear (48 -> 12)\n",
       "          (dropout): Dropout (p = 0)\n",
       "        )\n",
       "        (layer_normalization_self): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_enc): LayerNormalization (\n",
       "        )\n",
       "        (layer_normalization_feed): LayerNormalization (\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tgt_word_proj): Linear (12 -> 2926)\n",
       "  (dropout): Dropout (p = 0)\n",
       ")"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
