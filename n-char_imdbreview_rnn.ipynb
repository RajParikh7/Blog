{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"imdb_full.pkl\"\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array( [i for i in map(len, trn)] )\n",
    "print(lens.shape)\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = tf.contrib.keras.preprocessing.sequence.pad_sequences( trn , maxlen=500 )\n",
    "test = tf.contrib.keras.preprocessing.sequence.pad_sequences( test , maxlen=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(trn.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False , valx = None , valy = None):\n",
    "\n",
    "    Xd = np.array(Xd)\n",
    "    yd = np.array(yd)\n",
    "  \n",
    "    correct_prediction = tf.equal( y , predict )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    variables = [loss_val,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in tqdm_notebook( range(epochs) , leave=False ):\n",
    "        \n",
    "        correct = 0\n",
    "        losses = []\n",
    "        \n",
    "        for i in tqdm_notebook( range(int(math.ceil(Xd.shape[0]/batch_size))) , leave=False):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            feed_dict = {x: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                \n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if valx is not None:\n",
    "            temp1 = session.run(correct_prediction,feed_dict = {x:valx,y:valy,is_training:False})\n",
    "            print(\"Validation results here: \",np.sum(temp1)/valx.shape[0])\n",
    "        \n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2a3520c1034054afea1a7794ed7a5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e94969f90b49b583f1409e6a8a0158"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: with minibatch training loss = 0.688 and accuracy of 0.62\n",
      "Iteration 100: with minibatch training loss = 0.525 and accuracy of 0.8\n",
      "Iteration 200: with minibatch training loss = 0.256 and accuracy of 0.86\n",
      "Iteration 300: with minibatch training loss = 0.325 and accuracy of 0.8\n",
      "Epoch 1, Overall loss = 0.42 and accuracy of 0.781\n",
      "Validation results here:  0.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc57a47ed64cb6a7be34504603bfcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400: with minibatch training loss = 0.252 and accuracy of 0.89\n",
      "Iteration 500: with minibatch training loss = 0.176 and accuracy of 0.92\n",
      "Iteration 600: with minibatch training loss = 0.123 and accuracy of 0.97\n",
      "Iteration 700: with minibatch training loss = 0.0701 and accuracy of 0.98\n",
      "Epoch 2, Overall loss = 0.152 and accuracy of 0.944\n",
      "Validation results here:  0.878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder( tf.int32 , [None , 500 ])\n",
    "y = tf.placeholder( tf.int32 , [None] )\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "emb = tf.get_variable( \"emb\" , shape = [vocab_size , 32] , dtype=tf.float32 )\n",
    "\n",
    "emb_lookup = tf.nn.embedding_lookup( params=emb , ids= x)\n",
    "\n",
    "p1 = tf.reshape( emb_lookup , [-1,500*32] )\n",
    "p2 = tf.layers.dense( p1, 70 , activation = tf.nn.relu )\n",
    "p3 = tf.layers.dropout( p2 , .5 )\n",
    "p4 = tf.layers.dense( p3 , 1 , activation = tf.nn.sigmoid )\n",
    "p4 = tf.reshape( p4 , [-1] )\n",
    "\n",
    "pred = tf.cast( p4 >= 0.5 , tf.int32 )\n",
    "\n",
    "loss = tf.losses.log_loss(labels=y,predictions=p4)\n",
    "mean_loss = tf.reduce_mean( loss )\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    run_model( sess,pred,mean_loss,trn,labels_train,epochs=2,training=train_op,valx=test[:500],valy=labels_test[:500] )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc72cdecb554543b14eeed12c261e9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3423725eee4ff99f667ba1a9452b82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: with minibatch training loss = 0.691 and accuracy of 0.62\n",
      "Iteration 100: with minibatch training loss = 0.354 and accuracy of 0.88\n",
      "Iteration 200: with minibatch training loss = 0.414 and accuracy of 0.81\n",
      "Iteration 300: with minibatch training loss = 0.414 and accuracy of 0.84\n",
      "Iteration 400: with minibatch training loss = 0.336 and accuracy of 0.81\n",
      "Iteration 500: with minibatch training loss = 0.215 and accuracy of 0.88\n",
      "Iteration 600: with minibatch training loss = 0.439 and accuracy of 0.91\n",
      "Iteration 700: with minibatch training loss = 0.525 and accuracy of 0.78\n",
      "Epoch 1, Overall loss = 0.353 and accuracy of 0.849\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78815f5f2d294a859dc76b4dd94b1225"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800: with minibatch training loss = 0.125 and accuracy of 0.97\n",
      "Iteration 900: with minibatch training loss = 0.0647 and accuracy of 0.97\n",
      "Iteration 1000: with minibatch training loss = 0.186 and accuracy of 0.91\n",
      "Iteration 1100: with minibatch training loss = 0.485 and accuracy of 0.88\n",
      "Iteration 1200: with minibatch training loss = 0.313 and accuracy of 0.78\n",
      "Iteration 1300: with minibatch training loss = 0.232 and accuracy of 0.94\n",
      "Iteration 1400: with minibatch training loss = 0.185 and accuracy of 0.97\n",
      "Iteration 1500: with minibatch training loss = 0.0769 and accuracy of 0.97\n",
      "Epoch 2, Overall loss = 0.242 and accuracy of 0.905\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f34ef958e041bc9e74f4bc46769b27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a975a77b02403abccdc485571c6767"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Overall loss = 0.451 and accuracy of 0.843\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder( tf.int32 , [None , 500 ])\n",
    "y = tf.placeholder( tf.int32 , [None] )\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "emb = tf.get_variable( \"emb\" , shape = [vocab_size , 32] , dtype=tf.float32 )\n",
    "\n",
    "emb_lookup = tf.nn.embedding_lookup( params=emb , ids= x)\n",
    "\n",
    "p1 = tf.layers.dropout( emb_lookup , 0.2 )\n",
    "p2 = tf.layers.conv1d( p1 , 64 , 5 , activation= tf.nn.relu , padding='same' )\n",
    "p3 = tf.layers.dropout( p2 , 0.2 )\n",
    "p4 = tf.layers.max_pooling1d( p3 , 2 , 2 )\n",
    "\n",
    "p5 = tf.reshape( p4 , [-1,250*64] )\n",
    "p6 = tf.layers.dense( p5, 100 , activation = tf.nn.relu )\n",
    "p7 = tf.layers.dropout( p6 , .7 )\n",
    "p8 = tf.layers.dense( p7 , 1 , activation = tf.nn.sigmoid )\n",
    "p9 = tf.reshape( p8 , [-1] )\n",
    "\n",
    "pred = tf.cast( p9 >= 0.5 , tf.int32 )\n",
    "\n",
    "loss = tf.losses.log_loss(labels=y,predictions=p9)\n",
    "mean_loss = tf.reduce_mean( loss )\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    \n",
    "    run_model( sess,pred,mean_loss,trn,labels_train,epochs=2,\n",
    "              training=train_op, batch_size = 32 )\n",
    "    run_model( sess,pred,mean_loss,test,labels_test,epochs=1,batch_size=32)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c1acd543641e4a2e39d0362d3a658"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc9e698b3f642e7b560fa76517a5fe7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070108c01b8a41d7a4fa458bfce0cf9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: with minibatch training loss = 0.693 and accuracy of 0.59\n",
      "Iteration 100: with minibatch training loss = 0.418 and accuracy of 0.84\n",
      "Iteration 200: with minibatch training loss = 0.503 and accuracy of 0.81\n",
      "Iteration 300: with minibatch training loss = 0.189 and accuracy of 0.92\n",
      "Epoch 1, Overall loss = 0.41 and accuracy of 0.806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97706c0ea7334c3c9bce5d7ce62e5f3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2a0f227764474bd4cecb8b9fcea31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Overall loss = 0.292 and accuracy of 0.882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc74e080bf04e8296acca9cfdf808e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc25406c258042ad9417dfa81bea846f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: with minibatch training loss = 0.229 and accuracy of 0.89\n",
      "Iteration 100: with minibatch training loss = 0.154 and accuracy of 0.97\n",
      "Iteration 200: with minibatch training loss = 0.295 and accuracy of 0.91\n",
      "Iteration 300: with minibatch training loss = 0.162 and accuracy of 0.94\n",
      "Epoch 1, Overall loss = 0.223 and accuracy of 0.914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512722e0abfe4e63bd4ceda2b3335d7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25844940abb844d68b73aede5ca00bf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Overall loss = 0.269 and accuracy of 0.892\n",
      "\r"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder( tf.int32 , [None , 500 ])\n",
    "y = tf.placeholder( tf.int32 , [None] )\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "emb = tf.get_variable( \"emb\" , shape = [vocab_size , 32] , dtype=tf.float32 )\n",
    "emb_lookup = tf.nn.embedding_lookup( params=emb , ids= x)\n",
    "data = tf.layers.dropout( emb_lookup , 0.2 )\n",
    "\n",
    "# lstmCell = tf.contrib.rnn.BasicLSTMCell(150)\n",
    "# lstmCell_dr = tf.contrib.rnn.DropoutWrapper( cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "cells = []\n",
    "for _ in range(3):\n",
    "    cell = tf.contrib.rnn.GRUCell(64)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper( cell, output_keep_prob=0.75)\n",
    "    cells.append(cell)\n",
    "    \n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "value, _ = tf.nn.dynamic_rnn( cell , data, dtype=tf.float32)\n",
    "\n",
    "value = tf.transpose( value , [1, 0, 2])\n",
    "\n",
    "partitions = np.zeros( value.get_shape()[0] )\n",
    "partitions[-1] = 1\n",
    "\n",
    "checking = tf.dynamic_partition( value , partitions , 2 )\n",
    "# something = tf.shape(checking)\n",
    "\n",
    "last = tf.reshape( checking[1] , [-1,64])\n",
    "# last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "\n",
    "p1 = tf.layers.dense( last , 1 , activation=tf.nn.sigmoid )\n",
    "p2 = tf.reshape( p1 , [-1] )\n",
    "\n",
    "pred = tf.cast( p2 >= 0.5 , tf.int32 )\n",
    "\n",
    "loss = tf.losses.log_loss(labels=y,predictions=p2)\n",
    "mean_loss = tf.reduce_mean( loss )\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "#     a = sess.run( anoter , feed_dict = {x:trn[:100] , y:labels_train[:100] , is_training : True })\n",
    "#     print(a.shape)\n",
    "    \n",
    "    \n",
    "# #     print(__.shape)\n",
    "    for i in tqdm_notebook( range(2) ,leave = False ):\n",
    "        run_model( sess,pred,mean_loss,trn,labels_train,epochs=1,\n",
    "                  training=train_op, batch_size = 64 )\n",
    "        run_model( sess,pred,mean_loss,test,labels_test,epochs=1,batch_size=64)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ^ the encoded context vector gives us the best result as it intuitively should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4-th char generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "text = open(\"nietzsche.txt\").read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join( [indices_char[i] for i in idx[:70]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600897,), (600897,), (600897,), (600897,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array(c1_dat)\n",
    "x2 = np.array(c2_dat)\n",
    "x3 = np.array(c3_dat)\n",
    "\n",
    "y = np.array(c4_dat)\n",
    "x1.shape , x2.shape , x3.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "em_matrix = tf.get_variable( \"em_matrix\" , shape = [vocab_size,n_fac] , dtype=tf.float32 )\n",
    "\n",
    "in1 = tf.placeholder( tf.int32 , shape = [None] )\n",
    "in2 = tf.placeholder( tf.int32 , shape = [None] )\n",
    "in3 = tf.placeholder( tf.int32 , shape = [None] )\n",
    "\n",
    "out = tf.placeholder( tf.int64 , shape = [None] )\n",
    "\n",
    "i1 = tf.nn.embedding_lookup( em_matrix , in1)\n",
    "i2 = tf.nn.embedding_lookup( em_matrix , in2)\n",
    "i3 = tf.nn.embedding_lookup( em_matrix , in3)\n",
    "\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "h1 = tf.layers.dense( i1 , 20 , activation=tf.nn.relu , name = \"in_hidden\" )\n",
    "t1 = tf.layers.dense( i2 , 20 , activation=tf.nn.relu , name = \"in_hidden\" , reuse=True)\n",
    "t2 = tf.layers.dense( i3 , 20 , activation=tf.nn.relu , name = \"in_hidden\" , reuse=True)\n",
    "\n",
    "t3 = tf.layers.dense( h1 , 20 , activation=tf.nn.tanh , name = \"hidden_hidden\" ) + t1\n",
    "t4 = tf.layers.dense( t3 , 20 , activation=tf.nn.tanh , name = \"hidden_hidden\" , reuse=True ) + t2\n",
    "\n",
    "t5 = tf.layers.dense( t4 , vocab_size , name = \"hidden_out\" )\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy( tf.one_hot(out,vocab_size) , t5 )\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_index = tf.argmax(t5,axis=1)\n",
    "correct = tf.reduce_sum( tf.cast( tf.equal( correct_index , out ) , dtype=tf.float32 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(q1,q2,q3,q4,session,l,t,c,saver,batch_size=64,epoch=5):\n",
    "    \n",
    "    train_indices = np.arange( q1.shape[0] )\n",
    "    np.random.shuffle( train_indices )\n",
    "    \n",
    "    for e in tqdm_notebook( range(epoch) ):\n",
    "        \n",
    "        correct = 0\n",
    "        losses = []\n",
    "        for cc in tqdm_notebook( range(int(math.ceil(q1.shape[0]/batch_size))) ):\n",
    "            \n",
    "            start_idx = (cc*batch_size)%q1.shape[0]\n",
    "            idx = train_indices[ start_idx : start_idx+batch_size]\n",
    "            \n",
    "            feed_dict = { in1:q1[idx] , in2:q2[idx] , in3:q3[idx] , out:q4[idx] , is_training: True } \n",
    "            actual_batch_size = q1[idx].shape[0]\n",
    "            \n",
    "            lr, corr, _ = session.run( [l,c,t] , feed_dict=feed_dict )\n",
    "            losses.append(lr*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "        \n",
    "        saver.save(sess, \"tmp/first_rnn\",global_step = e)\n",
    "        total_correct = correct/q1.shape[0]\n",
    "        total_loss = np.sum(losses)/q1.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\".format(total_loss,total_correct,e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9b4aa9039045c4b4137842141da493"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ffe83d50974cd0b853db1a79525afc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Overall loss = 2.43 and accuracy of 0.315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2043d296bee74f8e88ad70ba336b0521"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Overall loss = 2.29 and accuracy of 0.343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5408b4a90413cb6beadb5bf3b93f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Overall loss = 2.26 and accuracy of 0.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     saver.restore(sess, \"tmp/first_rnn-5\")\n",
    "#     for i in range(2):\n",
    "#         _,__,lo = sess.run( [train_op,correct,loss] ,  feed_dict={ in1:x1[:1000] , in2:x2[:1000] , in3:x3[:1000] , out:y[:1000] , is_training: True } )\n",
    "#         print( __,lo )\n",
    "    run_model(x1,x2,x3,y,sess,loss,train_op,correct,saver,batch_size=64,epoch=3)\n",
    "#     prediction(\"men\",sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(te,sess):\n",
    "    temp = [ char_indices[i] for i in te ]\n",
    "    tep = sess.run( correct_index , feed_dict = { in1 : temp[:1] ,in2 : temp[1:2] , in3 : temp[2:3] ,is_training:False } )\n",
    "    print( indices_char[tep[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 71, 58, 59, 54, 56, 58]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[char_indices[i] for i in \"Preface\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "em_matrix = tf.get_variable( \"em_matrix\" , shape = [vocab_size,n_fac] , dtype=tf.float32 )\n",
    "\n",
    "in1 = tf.placeholder( tf.int64 , shape = [None,8] )\n",
    "out = tf.placeholder( tf.int64 , shape = [None] )\n",
    "\n",
    "i1 = tf.nn.embedding_lookup( em_matrix , in1)\n",
    "\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(13)\n",
    "value,_ = tf.nn.dynamic_rnn(cell,i1,dtype=tf.float32)\n",
    "\n",
    "value = tf.transpose( value , [1, 0, 2])\n",
    "\n",
    "partitions = np.zeros( value.get_shape()[0] )\n",
    "partitions[-1] = 1\n",
    "\n",
    "part = tf.dynamic_partition( value , partitions , 2 )\n",
    "\n",
    "sque = tf.squeeze(part[1],axis=[0])\n",
    "t5 = tf.layers.dense( sque , vocab_size )\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy( tf.one_hot(out,vocab_size) , t5 )\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_index = tf.argmax(t5,axis=1)\n",
    "correct = tf.reduce_sum( tf.cast( tf.equal( correct_index , out ) , dtype=tf.float32 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/second_rnn-5\n",
      "[73]\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"tmp/second_rnn-5\")\n",
    "#     for i in range(10):\n",
    "#         _,__,lo = sess.run( [train_op,correct,loss] ,  feed_dict={ in1:X[:1000], out:y[:1000] , is_training: True } )\n",
    "#         print( __,lo )\n",
    "#     run_model_2(X,y,sess,loss,train_op,correct,saver,batch_size=256,epoch=10)\n",
    "    prediction_2( 'it is no',sess)\n",
    "#     _ = sess.run(sque,feed_dict={in1:X[:12],out:y[:12]})\n",
    "#     print( _.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs)] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 600892)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array( c_in_dat ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [idx[i+cs] for i in range(0, len(idx)-1-cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array( c_in_dat).T\n",
    "y = np.array( c_out_dat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600892, 8), (600892,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model_2(q1,q4,session,l,t,c,saver,batch_size=64,epoch=5):\n",
    "    \n",
    "    train_indices = np.arange( q1.shape[0] )\n",
    "    np.random.shuffle( train_indices )\n",
    "    \n",
    "    for e in tqdm_notebook( range(epoch) ):\n",
    "        \n",
    "        correct = 0\n",
    "        losses = []\n",
    "        for cc in tqdm_notebook( range(int(math.ceil(q1.shape[0]/batch_size))) ):\n",
    "            \n",
    "            start_idx = (cc*batch_size)%q1.shape[0]\n",
    "            idx = train_indices[ start_idx : start_idx+batch_size]\n",
    "            \n",
    "            feed_dict = { in1:q1[idx] , out:q4[idx] , is_training: True } \n",
    "            actual_batch_size = q1[idx].shape[0]\n",
    "            \n",
    "            lr, corr, _ = session.run( [l,c,t] , feed_dict=feed_dict )\n",
    "            losses.append(lr*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "        \n",
    "        saver.save(sess, \"tmp/second_rnn\",global_step = e)\n",
    "        total_correct = correct/q1.shape[0]\n",
    "        total_loss = np.sum(losses)/q1.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\".format(total_loss,total_correct,e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_2(te,sess):\n",
    "    temp = [ char_indices[i] for i in te ]\n",
    "    tep = sess.run( correct_index , feed_dict = { in1 : [temp] ,is_training:False } )\n",
    "    print(tep)\n",
    "    print( indices_char[tep[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+n] for i in range(1, len(idx)-cs)] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array( c_out_dat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600892, 8)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "em_matrix = tf.get_variable( \"em_matrix\" , shape = [vocab_size,n_fac] , dtype=tf.float32 )\n",
    "\n",
    "in1 = tf.placeholder( tf.int64 , shape = [None,8] )\n",
    "out = tf.placeholder( tf.int64 , shape = [None,8] )\n",
    "\n",
    "i1 = tf.nn.embedding_lookup( em_matrix , in1)\n",
    "\n",
    "is_training = tf.placeholder( tf.bool )\n",
    "\n",
    "tf.nn.rnn_cell()\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(13)\n",
    "\n",
    "value , _ = tf.nn.dynamic_rnn(cell,i1,dtype=tf.float32)\n",
    "\n",
    "t5 = tf.layers.dense( value , vocab_size )\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy( tf.one_hot(out,vocab_size) , t5 )\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "correct_index = tf.argmax(t5,axis=2)\n",
    "correct = tf.reduce_sum( tf.cast( tf.equal( correct_index , out ) , dtype=tf.float32 ) ) / 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model_3(q1,q4,session,l,t,c,saver,batch_size=64,epoch=5):\n",
    "    \n",
    "    train_indices = np.arange( q1.shape[0] )\n",
    "#     np.random.shuffle( train_indices )\n",
    "    \n",
    "    for e in tqdm_notebook( range(epoch) ):\n",
    "        \n",
    "        correct = 0\n",
    "        losses = []\n",
    "        for cc in tqdm_notebook( range(int(math.ceil(q1.shape[0]/batch_size))) ):\n",
    "            \n",
    "            start_idx = (cc*batch_size)%q1.shape[0]\n",
    "            idx = train_indices[ start_idx : start_idx+batch_size]\n",
    "            \n",
    "            feed_dict = { in1:q1[idx] , out:q4[idx] , is_training: True } \n",
    "            actual_batch_size = q1[idx].shape[0]\n",
    "            \n",
    "            lr, corr, tr_op  = session.run( [l,c,t] , feed_dict=feed_dict )\n",
    "            losses.append(lr*actual_batch_size)\n",
    "            correct += corr\n",
    "        \n",
    "        saver.save(sess, \"tmp/third_rnn\",global_step = e)\n",
    "        total_correct = correct/q1.shape[0]\n",
    "        total_loss = np.sum(losses)/q1.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\".format(total_loss,total_correct,e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_3(te,sess):\n",
    "    print( list(te) )\n",
    "    temp = [ char_indices[i] for i in te ]\n",
    "    tep = sess.run( correct_index , feed_dict = { in1 : [temp] ,is_training:False } )\n",
    "    print( [indices_char[j] for j in tep[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/third_rnn-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af4d969691f4248a2ddb65c8f9c30ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1838a1939347b58b376a07257df8ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Overall loss = 2.17 and accuracy of 0.37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"tmp/third_rnn-4\")\n",
    "#     for i in range(10):\n",
    "#         _,__,lo = sess.run( [train_op,correct,loss] ,  feed_dict={ in1:X[:1000], out:y[:1000] , is_training: True } )\n",
    "#         print( __,lo )\n",
    "    run_model_3(X,y,sess,loss,train_op,correct,saver,batch_size=64,epoch=1)\n",
    "#     prediction_3( ' this is',sess)\n",
    "#     _ = sess.run(sque,feed_dict={in1:X[:12],out:y[:12]})\n",
    "#     print( _.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
